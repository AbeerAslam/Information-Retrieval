state abstraction reinforcement learning by eliminating useless dimension reinforcement learning state abstraction intelligent agent complexity reduction q learning and other linear dynamic learning algorithm subject bellman curse of dimensionality any realistic learning problem this paper introduces framework satisficing state abstraction one that reduces state dimensionality improving convergence and reducing computational and memory resource by eliminating useless state dimension statistical parameter that dependent state and q value identify relevance of given state space task space and allow state element that contribute least task learning discarded empirical result of applying state abstraction canonical single agent path planning task and more difficult multi agent foraging problem demonstrate utility of proposed method improving learning convergence and performance resource constrained learning problem