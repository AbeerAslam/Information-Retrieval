anytime exploitation of straggler synchronous stochastic gradient descent distributed sgd stochastic gradient descent parallelized sgd straggler this paper propose an approach parallelizing synchronous stochastic gradient descent sgd that term anytime gradient anytime gradient is designed exploit work completed by slow compute node or straggler many approach work completed by these node while only partial is discarded completely maintain synchronization our approach each computational epoch is of fixed duration and end of each epoch worker send updated parameter vector master mode combination master weight each update by amount of work done anytime gradient scheme is robust both persistent and non persistent straggler and requires prior knowledge about processor ability show that scheme effectively exploit straggler and outperforms existing method