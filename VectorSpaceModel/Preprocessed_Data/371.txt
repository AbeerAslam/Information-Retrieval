incomplete dot product dynamic computation scaling neural network inference machine learning approximate computing iot deep learning convolutional neural network propose use of incomplete dot product idp dynamically adjust number of input channel used each layer of convolutional neural network during feedforward inference idp add monotonically non increasing coefficient referred profile channel during training profile order contribution of each channel non increasing order inference time number of channel used dynamically adjusted trade off accuracy lowered power consumption and reduced latency by selecting only beginning subset of channel this approach allows single network dynamically scale over computation range opposed training and deploying multiple network support different level of computation scaling additionally extend notion multiple profile each optimized some specific range of computation scaling present experiment computation and accuracy trade offs of idp popular image classification model and datasets demonstrate that mnist and cifar 10 idp reduces computation significantly e g by 75 without significantly compromising accuracy argue that idp provides convenient and effective mean device lower computation cost dynamically reflect current computation budget of system example vgg 16 with 50 idp using only first 50 of channel achieves 70 accuracy cifar 10 dataset compared standard network which achieves only 35 accuracy when using reduced channel set