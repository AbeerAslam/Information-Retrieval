cyclic contrastive divergence learning algorithm high order rbms high order rbms cyclic contrastive divergence learning gradient approximation convergence upper bound restricted boltzmann machine rbm special case of general boltzmann machine and typical probabilistic graphical model attracted much attention recent year due it powerful ability extracting feature and representing distribution underlying training data most commonly used algorithm learning rbms is called contrastive divergence cd proposed by hinton which start markov chain data point and run chain only few iteration get low variance estimator however when referring high order rbm since there interaction among it visible layer gradient approximation via cd learning usually becomes far from log likelihood gradient and even may cause cd learning fall into infinite loop with high reconstruction error this paper new algorithm named cyclic contrastive divergence ccd is introduced learning high order rbms unlike standard cd algorithm ccd update parameter according each visible layer turn by borrowing idea of cyclic block coordinate descent method evaluate performance of proposed ccd algorithm regarding high order rbms learning both algorithm ccd and standard cd theoretically analyzed including convergence estimate upper bound and both bias comparison from which superiority of ccd learning is revealed experiment of handwritten digit classification task mnist dataset performed experimental result show that ccd is more applicable and consistently outperforms standard cd both convergent speed and performance