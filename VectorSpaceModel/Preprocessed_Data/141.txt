sparse temporal difference learning via alternating direction method of multiplier approximation algorithm context optimization function approximation learning artificial intelligence prediction algorithm linear programming recent work off line reinforcement learning focused efficient algorithm incorporate feature selection via l1 regularization into bellman operator fixed point estimator these development now mean that over fitting avoided when number of sample is small compared number of feature however it remains unclear whether existing algorithm ability offer good approximation task of policy evaluation and improvement this paper propose new algorithm approximating fixed point based alternating direction method of multiplier admm demonstrate with experimental result that proposed algorithm is more stable policy iteration compared prior work furthermore also derive theoretical result that state proposed algorithm obtains solution which satisfies optimality condition fixed point problem