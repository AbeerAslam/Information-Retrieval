assessing threat of adversarial example deep neural network neural network security agriculture training machine learning mimic camera deep neural network facing potential security threat from adversarial example input that look normal but cause an incorrect classification by deep neural network example proposed threat could result hand written digit scanned check being incorrectly classified but looking normal when human see them this research assesses extent which adversarial example pose security threat when one considers normal image acquisition process this process is mimicked by simulating transformation that normally occur of acquiring image real world application such using scanner acquire digit check amount or using camera an autonomous car these small transformation negate effect of carefully crafted perturbation of adversarial example resulting correct classification by deep neural network thus just acquiring image decrease potential impact of proposed security threat also show that already widely used process of averaging over multiple crop neutralizes most adversarial example normal preprocessing such text binarization almost completely neutralizes adversarial example this is first paper show that text driven classification adversarial example an academic curiosity not security threat